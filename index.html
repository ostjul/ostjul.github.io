<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Julian Ost">

    <title>Julian Ost</title>

    <link rel="shortcut icon" href="assets/favicon-princeton/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

    <script>
        // Add this JavaScript to handle email protection
        function handleEmailClick() {
            // Reconstruct email from parts to avoid scraping
            const username = 'julian.ost';  // Replace with your email username
            const domain = 'princeton.edu';    // Replace with your email domain
            const email = username + '@' + domain;
            
            // Copy to clipboard
            navigator.clipboard.writeText(email).then(() => {
                // Show notification
                const notification = document.createElement('div');
                notification.className = 'copy-notification';
                notification.textContent = 'Email address copied to clipboard!';
                document.body.appendChild(notification);
                notification.style.display = 'block';
                
                // Remove notification after animation
                setTimeout(() => {
                    notification.style.display = 'none';
                    notification.remove();
                }, 2000);
            });
        }
        </script>
</head>
<body>
    <div class="container">
        <header>
            <div class= "bio">
                <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                    <tr style="padding:0px">
                        <td style="padding:1.5%;width:75%;vertical-align:middle">
                            <h1><b>Julian</b> Ost</h1>
                            <p class="subtitle">PhD Candidate at Princeton University | 3D Generation & Inverse Rendering</p>
                            <br>

                        </td>
                        <td style="padding:1.5%;width:20%;max-width:12%;vertical-align:top">
                            <a href="assets/profile_picture_transparent.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile picture" src="assets/profile_picture_transparent.png" class="hoverZoomLink"></a>
                        </td>
                    </tr>
                    </tbody>
                </table>

                <p> I'm a PhD candidate in Computer Science at the 
                    <a href="https://light.princeton.edu/" class="external_link"> <b>Princeton Computational Imaging Lab </b></a>
                    advised by Felix Heide. My primary research interests lie at the intersection of computer vision and computer graphics, particularly in 3D generation and inverse rendering for perception. 
                    <br>
                    I graduated with a B.Sc. in Mechanical Engineering and a M.Sc. in Robotics from the Technical University of Munich (TUM). I work at Torc Robotics Europe (formerly at Algolux) on data driven simulation for autonomous driving.
                </p>           
            </div>
            <div class="social-links">
                <a href="https://scholar.google.com/citations?user=mUbWwU4AAAAJ" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14Zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5Z"/>
                    </svg>
                    Scholar
                </a>
                <a href="https://bsky.app/profile/ostjul.bsky.social" class="social-button">
                    <svg class="social-icon" viewBox="0 0 600 530" fill="currentColor">
                        <path d="m135.72 44.03c66.496 49.921 138.02 151.14 164.28 205.46 26.262-54.316 97.782-155.54 164.28-205.46 47.98-36.021 125.72-63.892 125.72 24.795 0 17.712-10.155 148.79-16.111 170.07-20.703 73.984-96.144 92.854-163.25 81.433 117.3 19.964 147.14 86.092 82.697 152.22-122.39 125.59-175.91-31.511-189.63-71.766-2.514-7.3797-3.6904-10.832-3.7077-7.8964-0.0174-2.9357-1.1937 0.51669-3.7077 7.8964-13.714 40.255-67.233 197.36-189.63 71.766-64.444-66.128-34.605-132.26 82.697-152.22-67.108 11.421-142.55-7.4491-163.25-81.433-5.9562-21.282-16.111-152.36-16.111-170.07 0-88.687 77.742-60.816 125.72-24.795z"/>
                    </svg>
                    Bluesky
                </a>
                <a href="https://www.linkedin.com/in/julian-ost" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                    LinkedIn
                </a>
                <a href="https://github.com/ostjul" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
                    </svg>
                    GitHub
                </a>
                <button onclick="handleEmailClick()" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                    </svg>
                    Email
                </button>
            </div>
        </header>

        <main>
            <article class="project">
                <div class="project-image">
                    <!-- <img src="https://light.princeton.edu/wp-content/uploads/2023/12/thumbnail_website_tracking.png" alt="Inverse Neural Rendering for Tracking"> -->
                    <a class="hover-project-image" href="#">
                        <!-- path/to/first/visible/image: -->
                        <img src="https://light.princeton.edu/wp-content/uploads/2023/12/thumbnail_website_tracking.png" />
                        <!-- path/to/hover/visible/image: -->
                        <img src="assets/inr_mot_teaser.gif" class="hide" />
                    </a>
                </div>
                <div class="project-content">
                    <h2>Inverse Neural Rendering for Explainable Multi-Object Tracking</h2>
                    <div class="tags">
                        <span class="tag">3D Multi-Object Tracking</span>
                        <span class="tag">Inverse Rendering</span>
                        <span class="tag">Explainability</span>
                    </div>
                    <p>We propose to recast 3D multi-object tracking (MOT) from RGB cameras as an Inverse Neural Rendering (INR) problem. By optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations, we retrieve the latents that best represent object instances in a given input image. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure cases, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method on automotive datasets that are completely unseen to our method and do not require fine-tuning.
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/inverse-rendering-tracking/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2024/04/ost_2024_neural_inverse_rendering_object_tracking.pdf">Paper (PDF)</a>
                        <!-- <a href="#">Code</a> -->
                    </div>
                </div>
            </article>


            <article class="project">
                <div class="project-image">
                    <!-- <img src="assets/new_teaser_pointLF.png" alt="Neural Point Light Fields"> -->
                    <a class="hover-project-image" href="#">
                        <!-- path/to/first/visible/image: -->
                        <img src="assets/new_teaser_pointLF.png" />
                        <!-- path/to/hover/visible/image: -->
                        <img src="assets/pointLF_teaser.gif" class="hide" />
                    </a>
                </div>
                <div class="project-content">
                    <h2>Neural Point Light Fields</h2>
                    <div class="tags">
                        <span class="conference-tag">CVPR 2022</span>
                        <span class="tag">Light Fields</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Point Clouds</span>
                    </div>
                    <p>
                        Neural Point Light Fields represent
                        scenes implicitly with a light field living on a sparse point
                        cloud. This approach combines the efficiency of point-based 
                        representations with the high-quality view synthesis capabilities 
                        of neural rendering, enabling realistic scene reconstruction from 
                        sparse inputs. Promoting sparse point clouds to neural implicit
                        light fields allows us to represent large scenes effectively
                        with only a single radiance evaluation.
                        <!-- As neural volumetric rendering methods
                        require dense sampling of the underlying functional scene
                        representation, at hundreds of samples along a ray cast
                        through the volume, they are fundamentally limited to small
                        scenes with the same objects projected to hundreds of train-
                        ing views.  -->
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/neural-point-light-fields/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2022/04/NeuralPointLightFields.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-point-light-fields">Code</a>
                    </div>
                </div>
            </article>

            <article class="project">
                <div class="project-image">
                    <!-- <img src="assets/scene_graph_isometric_small_colored.png" alt="Neural Scene Graphs visualization"> -->
                    <a class="hover-project-image" href="#">
                        <!-- path/to/first/visible/image: -->
                        <img src="assets/scene_graph_isometric_small_colored.png" />
                        <!-- path/to/hover/visible/image: -->
                        <img src="assets/dancing_cars.gif" class="hide" />
                    </a>
                </div>
                <div class="project-content">
                    <h2>Neural Scene Graphs for Dynamic Scenes</h2>
                    <div class="tags">
                        <span class="conference-tag">CVPR 2021 (Oral)</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Scene Understanding</span>
                        <span class="tag">3D Reconstruction</span>
                    </div>
                    <p>
                        Neural Scene Graphs combines traditional scene graph representations with neural networks. It presents a first neural rendering approach that decomposes dynamic multi-object scenes into a learned scene graph representation, that encode object transformation and radiance, to efficiently render novel arrangements and views of the scene. The proposed method is assessed on synthetic and real automotive data, validating that our approach learns dynamic scenes - only by observing a video of this scene - and allows for rendering novel photo-realistic views of novel scene compositions and manipulation with unseen sets of objects at unseen poses.</p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/neural-scene-graphs/">Project Page</a>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ost_Neural_Scene_Graphs_for_Dynamic_Scenes_CVPR_2021_paper.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-scene-graphs">Code</a>
                    </div>
                </div>
            </article>
        </main>
    </div>
</body>
</html>
