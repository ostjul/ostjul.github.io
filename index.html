<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Julian's Homepage</title>
    <style>
        /* CSS Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Typography */
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #363636;
            background-color: #f5f5f3;
        }

        /* Layout */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Header */
        header {
            padding: 4rem 0;
            border-bottom: 1px solid #ddd;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 300;
            letter-spacing: -0.5px;
            color: #232323;
            margin-bottom: 1rem;
        }

        .subtitle {
            font-size: 1.2rem;
            color: #565656;
            font-weight: 300;
        }

        /* Social Links */
        .social-links {
            display: flex;
            gap: 1rem;
            margin-top: 1.5rem;
        }
        
        .social-button {
            display: inline-flex;
            font-size: 0.85rem;
            align-items: center;
            gap: 0.5rem;
            padding: 0.2rem 0.8rem;
            border-radius: 2px;
            background-color: #e6e6e3;
            color: #333;
            text-decoration: none;
            transition: all 0.2s ease;
        }
        
        .social-button:hover {
            background-color: #e0e0e0;
            transform: translateY(-1px);
        }
        
        .social-icon {
            width: 20px;
            height: 20px;
        }        

        /* Projects */
        .project {
            display: grid;
            grid-template-columns: 1fr 2fr;
            gap: 2rem;
            padding: 3rem 0;
            border-bottom: 1px solid #ddd;
        }

        .project:last-child {
            border-bottom: none;
        }

        .project-image {
            width: 100%;
            aspect-ratio: 4/3;
            background-color: #e6e6e3;
            overflow: hidden;
        }

        .project-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .project-content h2 {
            font-size: 1.5rem;
            font-weight: 400;
            color: #232323;
            margin-bottom: 1rem;
        }

        .project-content p {
            color: #565656;
            margin-bottom: 1.5rem;
        }

        .project-links {
            display: flex;
            gap: 1rem;
        }

        .project-links a {
            text-decoration: none;
            color: #232323;
            padding: 0.5rem 0;
            position: relative;
        }

        .project-links a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 1px;
            background-color: currentColor;
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.3s ease;
        }

        .project-links a:hover::after {
            transform: scaleX(1);
        }

        .tags {
            display: flex;
            gap: 0.5rem;
            margin-bottom: 1rem;
            flex-wrap: wrap;
        }

        .tag {
            font-size: 0.85rem;
            padding: 0.2rem 0.8rem;
            background-color: #e6e6e3;
            border-radius: 2px;
            color: #565656;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .project {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }

            header {
                padding: 2rem 0;
            }

            .container {
                padding: 1rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Julian Ost</h1>
            <p class="subtitle">Ph. D. Candidate Princeton University | Inverse Rendering</p>
            <div class="social-links">
                <a href="https://www.linkedin.com/in/julian-ost" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                    LinkedIn
                </a>
                <a href="https://scholar.google.com/citations?user=mUbWwU4AAAAJ" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14Zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5Z"/>
                    </svg>
                    Google Scholar
                </a>
            </div>
        </header>

        <main>
            <article class="project">
                <div class="project-image">
                    <img src="https://light.princeton.edu/wp-content/uploads/2023/12/thumbnail_website_tracking.png" alt="Inverse Neural Rendering for Tracking">
                </div>
                <div class="project-content">
                    <h2>Inverse Neural Rendering for Explainable Multi-Object Tracking</h2>
                    <div class="tags">
                        <span class="tag">3D Multi-Object Tracking</span>
                        <span class="tag">Inverse Rendering</span>
                        <span class="tag">Explainability</span>
                    </div>
                    <p>We propose to recast 3D multi-object tracking (MOT) from RGB cameras as an Inverse Neural Rendering (INR) problem. By optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations, we retrieve the latents that best represent object instances in a given input image. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure cases, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method on automotive datasets that are completely unseen to our method and do not require fine-tuning.
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/inverse-rendering-tracking/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2024/04/ost_2024_neural_inverse_rendering_object_tracking.pdf">Paper (PDF)</a>
                        <!-- <a href="#">Code</a> -->
                    </div>
                </div>
            </article>


            <article class="project">
                <div class="project-image">
                    <img src="assets/new_teaser_pointLF.png" alt="Neural Point Light Fields">
                </div>
                <div class="project-content">
                    <h2>Neural Point Light Fields</h2>
                    <div class="tags">
                        <span class="tag">CVPR 2022</span>
                        <span class="tag">Light Fields</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Point Clouds</span>
                    </div>
                    <p>
                        Neural Point Light Fields represent
                        scenes implicitly with a light field living on a sparse point
                        cloud. This approach combines the efficiency of point-based 
                        representations with the high-quality view synthesis capabilities 
                        of neural rendering, enabling realistic scene reconstruction from 
                        sparse inputs. Promoting sparse point clouds to neural implicit
                        light fields allows us to represent large scenes effectively
                        with only a single radiance evaluation.
                        <!-- As neural volumetric rendering methods
                        require dense sampling of the underlying functional scene
                        representation, at hundreds of samples along a ray cast
                        through the volume, they are fundamentally limited to small
                        scenes with the same objects projected to hundreds of train-
                        ing views.  -->
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/neural-point-light-fields/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2022/04/NeuralPointLightFields.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-point-light-fields">Code</a>
                    </div>
                </div>
            </article>

            <article class="project">
                <div class="project-image">
                    <img src="assets/scene_graph_isometric_small_colored.png" alt="Neural Scene Graphs visualization">
                </div>
                <div class="project-content">
                    <h2>Neural Scene Graphs for Dynamic Scenes</h2>
                    <div class="tags">
                        <span class="tag">CVPR 2021 (Oral)</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Scene Understanding</span>
                        <span class="tag">3D Reconstruction</span>
                    </div>
                    <p>
                        Neural Scene Graphs combines traditional scene graph representations with neural networks. It presents a first neural rendering approach that decomposes dynamic multi-object scenes into a learned scene graph representation, that encode object transformation and radiance, to efficiently render novel arrangements and views of the scene. The proposed method is assessed on synthetic and real automotive data, validating that our approach learns dynamic scenes - only by observing a video of this scene - and allows for rendering novel photo-realistic views of novel scene compositions and manipulation with unseen sets of objects at unseen poses.</p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publications/">Project Page</a>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ost_Neural_Scene_Graphs_for_Dynamic_Scenes_CVPR_2021_paper.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-scene-graphs">Code</a>
                    </div>
                </div>
            </article>
        </main>
    </div>
</body>
</html>