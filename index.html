<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Julian's Homepage</title>
    <style>
        /* CSS Reset */
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        /* Typography */
        body {
            font-family:  Helvetica, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #363636;
            background-color: #f5f5f3;
        }

        p {
            font-family: -apple-system, Helvetica Light, Helvetica; 
            line-height: 1.6;
            color: #363636;
        }

        a.external_link:link  {
            color: #363636;
            font-weight: normal;
            text-decoration: none;
        }

        a.external_link:visited b {
            color: #363636;
            font-weight: normal;
            text-decoration: none;
        }

        a.external_link:hover {
            color: #f2ad70;
            font-weight: normal;
            text-decoration: none;
        }

        /* Layout */
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        /* Header */
        header {
            padding: 3rem 0;
            border-bottom: 1px solid #ddd;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 300;
            letter-spacing: -0.5px;
            color: #232323;
            margin-bottom: 1rem;
        }

        .subtitle {
            font-size: 1.2rem;
            color: #565656;
            font-weight: 300;
        }

        .bio
        {
            margin-top: 1.2rem;
            color: #565656;
            font-weight: 300;
        }

        /* Social Links */
        .social-links {
            display: flex;
            gap: 1rem;
            margin-top: 1.5rem;
        }
        
        .social-button {
            display: inline-flex;
            font-size: 0.85rem;
            align-items: center;
            gap: 0.5rem;
            padding: 0.2rem 0.8rem;
            border-radius: 2px;
            background-color: #e6e6e3;
            color: #333;
            text-decoration: none;
            transition: all 0.2s ease;
            border: none;
            font-size: inherit;
            font-family: inherit;
            cursor: pointer;
            /* Remove any default button styling */
            appearance: none;
            -webkit-appearance: none;
            -moz-appearance: none;
            outline: none;
            box-shadow: none;
            margin: 0;
            line-height: inherit;
        }
        
        .social-button:hover {
            background-color: #e0e0e0;
            transform: translateY(-1px);
        }
        
        .social-icon {
            width: 20px;
            height: 20px;
        }

        /* Styles for copy notification */
        .copy-notification {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background-color: #333;
            color: white;
            padding: 8px 16px;
            border-radius: 2px;
            display: none;
            animation: fadeInOut 2s ease;
        }
                
        @keyframes fadeInOut {
            0% { opacity: 0; }
            15% { opacity: 1; }
            85% { opacity: 1; }
            100% { opacity: 0; }
        }

        /* Projects */
        .project {
            display: grid;
            grid-template-columns: 1fr 2fr;
            gap: 2rem;
            padding: 3rem 0;
            border-bottom: 1px solid #ddd;
        }

        .project:last-child {
            border-bottom: none;
        }

        .project-image {
            width: 100%;
            aspect-ratio: 4/3;
            background-color: #e6e6e3;
            overflow: hidden;
        }

        .project-image img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        .project-content h2 {
            font-size: 1.5rem;
            font-weight: 400;
            color: #232323;
            margin-bottom: 1rem;
        }

        .project-content p {
            color: #565656;
            margin-bottom: 1.5rem;
        }

        .project-links {
            display: flex;
            gap: 1rem;
        }

        .project-links a {
            text-decoration: none;
            color: #232323;
            font-weight: 300;
            font-size: 1rem;
            padding: 0.5rem 0;
            position: relative;
        }

        .project-links a::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 0;
            width: 100%;
            height: 1px;
            background-color: currentColor;
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.3s ease;
        }

        .project-links a:hover::after {
            transform: scaleX(1);
        }

        .project-links a:visited {
            transform: scaleX(1);
            color: #363636;
        }

        .project-links a:hover {
            color: #f2ad70;
        }

        .tags {
            display: flex;
            gap: 0.5rem;
            margin-bottom: 1rem;
            flex-wrap: wrap;
        }

        .tag {
            font-size: 0.85rem;
            padding: 0.2rem 0.8rem;
            background-color: #e6e6e3;
            border-radius: 2px;
            color: #565656;
        }

        .conference-tag {
            font-size: 0.85rem;
            padding: 0.2rem 0.8rem;
            background-color: #fbd1ab;
            border-radius: 2px;
            color: #565656;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .project {
                grid-template-columns: 1fr;
                gap: 1.5rem;
            }

            header {
                padding: 2rem 0;
            }

            .container {
                padding: 1rem;
            }
        }
    </style>
    <script>
        // Add this JavaScript to handle email protection
        function handleEmailClick() {
            // Reconstruct email from parts to avoid scraping
            const username = 'julian.ost';  // Replace with your email username
            const domain = 'princeton.edu';    // Replace with your email domain
            const email = username + '@' + domain;
            
            // Copy to clipboard
            navigator.clipboard.writeText(email).then(() => {
                // Show notification
                const notification = document.createElement('div');
                notification.className = 'copy-notification';
                notification.textContent = 'Email address copied to clipboard!';
                document.body.appendChild(notification);
                notification.style.display = 'block';
                
                // Remove notification after animation
                setTimeout(() => {
                    notification.style.display = 'none';
                    notification.remove();
                }, 2000);
            });
        }
        </script>
</head>
<body>
    <div class="container">
        <header>
            <h1>Julian Ost</h1>
            <p class="subtitle">PhD Candidate at Princeton University | Inverse Rendering</p>
            <div class= "bio">
                <p> I'm a PhD candidate in Computer Science at the 
                    <a href="https://light.princeton.edu/" class="external_link"> <b>Princeton Computational Imaging Lab </b></a>
                     advised by Felix Heide. My primary areas of interest in research are in computer vision and graphics, especially inverse rendering for 3D perception. I graduated with a B.Sc. in Mechanical Engineering and a M.Sc. in Robotics from the Technical University of Munich (TUM). I work at Torc Robotics (fromerly at Algolux) data driven simulation for autonmous driving. </p>
            </div>
            <div class="social-links">
                <a href="https://www.linkedin.com/in/julian-ost" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                    LinkedIn
                </a>
                <a href="https://github.com/ostjul" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 0C5.374 0 0 5.373 0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23A11.509 11.509 0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/>
                    </svg>
                    GitHub
                </a>
                <a href="https://scholar.google.com/citations?user=mUbWwU4AAAAJ" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M12 24a7 7 0 1 1 0-14 7 7 0 0 1 0 14Zm0-24L0 9.5l4.838 3.94A8 8 0 0 1 12 9a8 8 0 0 1 7.162 4.44L24 9.5Z"/>
                    </svg>
                    Scholar
                </a>
                <button onclick="handleEmailClick()" class="social-button">
                    <svg class="social-icon" viewBox="0 0 24 24" fill="currentColor">
                        <path d="M20 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
                    </svg>
                    Email
                </button>
            </div>
        </header>

        <main>
            <article class="project">
                <div class="project-image">
                    <img src="https://light.princeton.edu/wp-content/uploads/2023/12/thumbnail_website_tracking.png" alt="Inverse Neural Rendering for Tracking">
                </div>
                <div class="project-content">
                    <h2>Inverse Neural Rendering for Explainable Multi-Object Tracking</h2>
                    <div class="tags">
                        <span class="tag">3D Multi-Object Tracking</span>
                        <span class="tag">Inverse Rendering</span>
                        <span class="tag">Explainability</span>
                    </div>
                    <p>We propose to recast 3D multi-object tracking (MOT) from RGB cameras as an Inverse Neural Rendering (INR) problem. By optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations, we retrieve the latents that best represent object instances in a given input image. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure cases, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method on automotive datasets that are completely unseen to our method and do not require fine-tuning.
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/inverse-rendering-tracking/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2024/04/ost_2024_neural_inverse_rendering_object_tracking.pdf">Paper (PDF)</a>
                        <!-- <a href="#">Code</a> -->
                    </div>
                </div>
            </article>


            <article class="project">
                <div class="project-image">
                    <img src="assets/new_teaser_pointLF.png" alt="Neural Point Light Fields">
                </div>
                <div class="project-content">
                    <h2>Neural Point Light Fields</h2>
                    <div class="tags">
                        <span class="conference-tag">CVPR 2022</span>
                        <span class="tag">Light Fields</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Point Clouds</span>
                    </div>
                    <p>
                        Neural Point Light Fields represent
                        scenes implicitly with a light field living on a sparse point
                        cloud. This approach combines the efficiency of point-based 
                        representations with the high-quality view synthesis capabilities 
                        of neural rendering, enabling realistic scene reconstruction from 
                        sparse inputs. Promoting sparse point clouds to neural implicit
                        light fields allows us to represent large scenes effectively
                        with only a single radiance evaluation.
                        <!-- As neural volumetric rendering methods
                        require dense sampling of the underlying functional scene
                        representation, at hundreds of samples along a ray cast
                        through the volume, they are fundamentally limited to small
                        scenes with the same objects projected to hundreds of train-
                        ing views.  -->
                    </p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/neural-point-light-fields/">Project Page</a>
                        <a href="https://light.princeton.edu/wp-content/uploads/2022/04/NeuralPointLightFields.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-point-light-fields">Code</a>
                    </div>
                </div>
            </article>

            <article class="project">
                <div class="project-image">
                    <img src="assets/scene_graph_isometric_small_colored.png" alt="Neural Scene Graphs visualization">
                </div>
                <div class="project-content">
                    <h2>Neural Scene Graphs for Dynamic Scenes</h2>
                    <div class="tags">
                        <span class="conference-tag">CVPR 2021 (Oral)</span>
                        <span class="tag">Neural Rendering</span>
                        <span class="tag">Scene Understanding</span>
                        <span class="tag">3D Reconstruction</span>
                    </div>
                    <p>
                        Neural Scene Graphs combines traditional scene graph representations with neural networks. It presents a first neural rendering approach that decomposes dynamic multi-object scenes into a learned scene graph representation, that encode object transformation and radiance, to efficiently render novel arrangements and views of the scene. The proposed method is assessed on synthetic and real automotive data, validating that our approach learns dynamic scenes - only by observing a video of this scene - and allows for rendering novel photo-realistic views of novel scene compositions and manipulation with unseen sets of objects at unseen poses.</p>
                    <div class="project-links">
                        <a href="https://light.princeton.edu/publication/neural-scene-graphs/">Project Page</a>
                        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ost_Neural_Scene_Graphs_for_Dynamic_Scenes_CVPR_2021_paper.pdf">Paper (PDF)</a>
                        <a href="https://github.com/princeton-computational-imaging/neural-scene-graphs">Code</a>
                    </div>
                </div>
            </article>
        </main>
    </div>
</body>
</html>
